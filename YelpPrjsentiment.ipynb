{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Big Data Final Project Yelp \n",
    "## Group Members: Haoning Liu, Jiaqi Chen, Mengqi Liu, Xiaolu Li"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://ip-172-31-60-156.ec2.internal:4040\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v2.4.3</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>yarn</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>BD4</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7fc44047f5c0>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder.appName(\"BD4\").getOrCreate()\n",
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sentiment analysis on review"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. Review data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = spark.read\\\n",
    "  .format('csv')\\\n",
    "  .option('header', 'true')\\\n",
    "  .option('inferSchema', 'true')\\\n",
    "  .load('s3://bigdataclasslhn/Prj_Yelp/review1.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "review=review.drop('_c0')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check schema"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- review_id: string (nullable = true)\n",
      " |-- user_id: string (nullable = true)\n",
      " |-- business_id: string (nullable = true)\n",
      " |-- stars: integer (nullable = true)\n",
      " |-- useful: integer (nullable = true)\n",
      " |-- funny: integer (nullable = true)\n",
      " |-- cool: integer (nullable = true)\n",
      " |-- text: string (nullable = true)\n",
      " |-- date: timestamp (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Check unique values for each column and whether data has been read in correct format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|         business_id|\n",
      "+--------------------+\n",
      "|E9QTQ4DOKo1UsGNmM...|\n",
      "|bo3SQVtErnMOqO6lk...|\n",
      "|uC3qwaxsOkdJzpOc0...|\n",
      "|mA27CG2U3ytmkxIGV...|\n",
      "|R_M4P9XetEM-aLE7e...|\n",
      "|ipFreSFhjClfNETuM...|\n",
      "|oIEmXWLtoh5blz-iw...|\n",
      "|f4mh1Y0rnvbJRfQ3j...|\n",
      "|eUI230JcFZLajIcDd...|\n",
      "|19m3NtbbP2VX-tDFJ...|\n",
      "|1UwaMUnVKeWcV14qv...|\n",
      "|FpFIAW_IEvASZBbus...|\n",
      "|smmLyq8f_YaxXPwZY...|\n",
      "|a1TTRvtMCDoxalOU6...|\n",
      "|7coCBjZNMJ48BD2ta...|\n",
      "|lt8IW9Bpy9GMeKGxy...|\n",
      "|cKwg6HFaLYXl7Ar0r...|\n",
      "|Cl-xl1vTUwHeaGgBx...|\n",
      "|DEBqmgxv2yhJ93LqG...|\n",
      "|nqBLuNgN1VrQhi6vU...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('business_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76755"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "review.select('business_id').distinct().count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|stars|\n",
      "+-----+\n",
      "| null|\n",
      "|    1|\n",
      "|    3|\n",
      "|    5|\n",
      "|    4|\n",
      "|    2|\n",
      "+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('stars').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|             user_id|\n",
      "+--------------------+\n",
      "|H0tfWQsGjEBuhXD4W...|\n",
      "|cJlM64tJEdMxirjfw...|\n",
      "|DgZO2UiUoAJQ5pw5v...|\n",
      "|UY5kjH85BrmN9YHIb...|\n",
      "|NFH6lgwwub14W-sR7...|\n",
      "|Gb9Y_f1xslY1mDvvH...|\n",
      "|D2ljL5ejuqpa4f8fn...|\n",
      "|llsbSKHnzAUPByQwz...|\n",
      "|XAja3Ed6Fa_lwZl_7...|\n",
      "|ft0uc4tJNjgwVa7F8...|\n",
      "|sJF4jbF5LyGWmkN8B...|\n",
      "|Inh6RQ9BjpV05V74O...|\n",
      "|ttc3_AtKQzrFlFdfs...|\n",
      "|2Ly_E_OZnJu8-fmpQ...|\n",
      "|QRqktJWBUi3HL7539...|\n",
      "|O9Dbdo2LZut3e8VqI...|\n",
      "|418W7Ufoz3eXSt3A1...|\n",
      "|2iuocD002C1y2f2f1...|\n",
      "|C16QOoBorPQwN1oeR...|\n",
      "|t5Neo4x7mL7j98bia...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('user_id').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+\n",
      "|useful|\n",
      "+------+\n",
      "|   148|\n",
      "|    31|\n",
      "|    85|\n",
      "|   251|\n",
      "|   808|\n",
      "|   137|\n",
      "|    65|\n",
      "|    53|\n",
      "|   255|\n",
      "|   970|\n",
      "|   133|\n",
      "|    78|\n",
      "|   362|\n",
      "|   108|\n",
      "|   155|\n",
      "|    34|\n",
      "|   193|\n",
      "|   101|\n",
      "|   126|\n",
      "|   115|\n",
      "+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('useful').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|funny|\n",
      "+-----+\n",
      "|  148|\n",
      "|   31|\n",
      "|   85|\n",
      "|  137|\n",
      "|   65|\n",
      "|   53|\n",
      "|  970|\n",
      "|  133|\n",
      "|   78|\n",
      "|  322|\n",
      "|  108|\n",
      "|  155|\n",
      "|   34|\n",
      "|  101|\n",
      "|  115|\n",
      "|   81|\n",
      "|   28|\n",
      "|  183|\n",
      "|  412|\n",
      "|   76|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('funny').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------+\n",
      "|               date|\n",
      "+-------------------+\n",
      "|2016-01-18 19:56:59|\n",
      "|2017-03-05 16:18:31|\n",
      "|2016-06-14 13:13:16|\n",
      "|2016-08-13 17:44:31|\n",
      "|2014-05-11 01:27:24|\n",
      "|2016-08-27 01:54:59|\n",
      "|2014-06-30 18:05:22|\n",
      "|2015-12-07 19:21:07|\n",
      "|2016-01-06 14:48:30|\n",
      "|2011-05-31 21:33:05|\n",
      "|2014-09-27 18:33:15|\n",
      "|2018-03-18 00:52:38|\n",
      "|2012-12-07 21:28:36|\n",
      "|2014-08-12 01:44:53|\n",
      "|2017-09-10 06:54:32|\n",
      "|2017-07-10 04:30:44|\n",
      "|2014-08-26 21:02:47|\n",
      "|2013-11-16 20:54:19|\n",
      "|2017-12-03 16:57:03|\n",
      "|2016-05-08 21:21:49|\n",
      "+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.select('date').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark import SparkContext \n",
    "from pyspark.sql import SQLContext\n",
    "from pyspark.sql.functions import count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "| null|     28|\n",
      "|    1| 368956|\n",
      "|    2| 201273|\n",
      "|    3| 276740|\n",
      "|    4| 552435|\n",
      "|    5|1100596|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.groupBy('stars').agg(count('review_id').alias('count')).sort('stars').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "review = review.filter(review.stars.isNotNull())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------+\n",
      "|stars|  count|\n",
      "+-----+-------+\n",
      "|    1| 368956|\n",
      "|    2| 201273|\n",
      "|    3| 276740|\n",
      "|    4| 552435|\n",
      "|    5|1100596|\n",
      "+-----+-------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review.groupBy('stars').agg(count('review_id').alias('count')).sort('stars').show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the summary table, we can see the majority of the reviews focuses on the positive sides. People tend to leave reviews when they have a good expression on their experience. Meanwhile, the number of extremely negative reviews (1 star) also much larger than the number of neutral reviews (2 or 3 stars)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprossing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Relabel the stars so that any reviews with 4 stars or above will be 1, which means positive, anything else is deemed to be 0,which means negative reviews. \n",
    "\n",
    "We begin to do text processings before tokenizing the text."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "# remove punctuation\n",
    "def remove_punctuation(text):\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    clean = regex.sub(\" \", str(text))\n",
    "    return clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# relabel target variable\n",
    "def convert_rate(rate):\n",
    "    rate = int(rate)\n",
    "    if rate >=4: \n",
    "        return 1\n",
    "    else: \n",
    "        return 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf, expr, concat, col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "punct_remover = udf(lambda x: remove_punctuation(x))\n",
    "rating_convert = udf(lambda x: convert_rate(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql.types import IntegerType\n",
    "# apply to review raw data\n",
    "review_df = review.select('review_id', punct_remover('text'), rating_convert('stars'))\n",
    "\n",
    "review_df = review_df.withColumnRenamed('<lambda>(text)', 'text')\\\n",
    "                     .withColumn('label', review_df[\"<lambda>(stars)\"].cast(IntegerType()))\\\n",
    "                     .drop('<lambda>(stars)')\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+\n",
      "|           review_id|                text|label|\n",
      "+--------------------+--------------------+-----+\n",
      "|Q1sbwvVQXV2734tPg...|Total bill for th...|    0|\n",
      "|GJXCdrto3ASJOqKeV...|I  adore  Travis ...|    1|\n",
      "|2TzJjDVDEuAW6MR5V...|I have to say tha...|    1|\n",
      "|yi0R0Ugj_xUx_Nek0...|Went in for a lun...|    1|\n",
      "|11a8sVPMUFtaC7_AB...|Today was my seco...|    0|\n",
      "|fdiNeiN_hoCxCMy2w...|I ll be the first...|    1|\n",
      "|G7XHMxG0bx9oBJNEC...|Tracy dessert had...|    0|\n",
      "|8e9HxxLjjqc9ez5ez...|This place has go...|    0|\n",
      "|qrffudO73zsslZbe8...|I was really look...|    0|\n",
      "|RS_GTIT6836bCaPy6...|It s a giant Best...|    0|\n",
      "|kbtscdyz6lvrtGjD1...|Like walking back...|    1|\n",
      "|-I5umRTkhw15RqpKM...|Walked in around ...|    0|\n",
      "|Z7wgXp98wYB57QdRY...|Wow  So surprised...|    1|\n",
      "|qlXw1JQ0UodW7qrmV...|Michael from Red ...|    1|\n",
      "|JVcjMhlavKKn3UIt9...|I cannot believe ...|    0|\n",
      "|svK3nBU7Rk8VfGorl...|You can t really ...|    1|\n",
      "|1wVA2-vQIuW_ClmXk...|Great lunch today...|    1|\n",
      "|6BnQwlxRn7ZuWdzni...|I love chinese fo...|    0|\n",
      "|rEITo90tpyKmEfNDp...|We ve been a huge...|    1|\n",
      "|4bUyL7lzoWzDZaJET...|Good selection of...|    0|\n",
      "+--------------------+--------------------+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tokenize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import *\n",
    "#make tokens and remove stopwords\n",
    "tok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "review_tokenized = tok.transform(review_df)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.corpus import stopwords\n",
    "#stop word remover\n",
    "stopword_rm = StopWordsRemover(inputCol='words', outputCol='words_nsw')\n",
    "review_tokenized = stopword_rm.transform(review_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|           review_id|                text|label|               words|           words_nsw|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "|Q1sbwvVQXV2734tPg...|Total bill for th...|    0|[total, bill, for...|[total, bill, hor...|\n",
      "|GJXCdrto3ASJOqKeV...|I  adore  Travis ...|    1|[i, , adore, , tr...|[, adore, , travi...|\n",
      "|2TzJjDVDEuAW6MR5V...|I have to say tha...|    1|[i, have, to, say...|[say, office, rea...|\n",
      "|yi0R0Ugj_xUx_Nek0...|Went in for a lun...|    1|[went, in, for, a...|[went, lunch, , s...|\n",
      "|11a8sVPMUFtaC7_AB...|Today was my seco...|    0|[today, was, my, ...|[today, second, t...|\n",
      "|fdiNeiN_hoCxCMy2w...|I ll be the first...|    1|[i, ll, be, the, ...|[ll, first, admit...|\n",
      "|G7XHMxG0bx9oBJNEC...|Tracy dessert had...|    0|[tracy, dessert, ...|[tracy, dessert, ...|\n",
      "|8e9HxxLjjqc9ez5ez...|This place has go...|    0|[this, place, has...|[place, gone, hil...|\n",
      "|qrffudO73zsslZbe8...|I was really look...|    0|[i, was, really, ...|[really, looking,...|\n",
      "|RS_GTIT6836bCaPy6...|It s a giant Best...|    0|[it, s, a, giant,...|[giant, best, buy...|\n",
      "|kbtscdyz6lvrtGjD1...|Like walking back...|    1|[like, walking, b...|[like, walking, b...|\n",
      "|-I5umRTkhw15RqpKM...|Walked in around ...|    0|[walked, in, arou...|[walked, around, ...|\n",
      "|Z7wgXp98wYB57QdRY...|Wow  So surprised...|    1|[wow, , so, surpr...|[wow, , surprised...|\n",
      "|qlXw1JQ0UodW7qrmV...|Michael from Red ...|    1|[michael, from, r...|[michael, red, ca...|\n",
      "|JVcjMhlavKKn3UIt9...|I cannot believe ...|    0|[i, cannot, belie...|[believe, things,...|\n",
      "|svK3nBU7Rk8VfGorl...|You can t really ...|    1|[you, can, t, rea...|[really, find, an...|\n",
      "|1wVA2-vQIuW_ClmXk...|Great lunch today...|    1|[great, lunch, to...|[great, lunch, to...|\n",
      "|6BnQwlxRn7ZuWdzni...|I love chinese fo...|    0|[i, love, chinese...|[love, chinese, f...|\n",
      "|rEITo90tpyKmEfNDp...|We ve been a huge...|    1|[we, ve, been, a,...|[ve, huge, slim, ...|\n",
      "|4bUyL7lzoWzDZaJET...|Good selection of...|    0|[good, selection,...|[good, selection,...|\n",
      "+--------------------+--------------------+-----+--------------------+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "review_tokenized.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trigrams with Frequency "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We decided to use trigram to conduct sentiment analysis and want to select trigram frequency larger than 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create 3 gram columns\n",
    "n = 3\n",
    "ngram = NGram(inputCol = 'words', outputCol = 'threegram', n = n)\n",
    "add_ngram = ngram.transform(review_tokenized)\n",
    "\n",
    "# generate the top frequent ngram\n",
    "ngrams = add_ngram.rdd.flatMap(lambda x: x[-1]).filter(lambda x: len(x.split())==n)\n",
    "ngram_tall = ngrams.map(lambda x: (x, 1))\\\n",
    "                      .reduceByKey(lambda x,y: x+y)\\\n",
    "                      .sortBy(lambda x: x[1], ascending=False)\\\n",
    "                      .filter(lambda x: x[1]>=10)\n",
    "ngram_list = ngram_tall.map(lambda x: x[0]).collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the word with selected ngram\n",
    "def ngram_concat(text):\n",
    "    textlower = text.lower()\n",
    "    for ngram in ngram_list:\n",
    "        return textlower.replace(ngram, ngram.replace(' ', '_'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "ngram_df = udf(lambda x: ngram_concat(x))\n",
    "ngram_df = review_tokenized.select(ngram_df('text'), 'label')\\\n",
    "                          .withColumnRenamed('<lambda>(text)', 'text')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenize and remove stop words with ngram\n",
    "tok = Tokenizer(inputCol=\"text\", outputCol=\"words\")\n",
    "review_tokenized = tok.transform(review_df)\n",
    "tokenized_ngram = tok.transform(ngram_df)\n",
    "tokenized_ngram = stopword_rm.transform(tokenized_ngram)\n",
    "\n",
    "stopword_rm = StopWordsRemover(inputCol='words', outputCol='words_nsw')\n",
    "review_tokenized = stopword_rm.transform(review_tokenized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# count vectorizer and tfidf\n",
    "cv = CountVectorizer(inputCol='words_nsw', outputCol='tf')\n",
    "cvModel = cv.fit(review_tokenized)\n",
    "count_vectorized = cvModel.transform(review_tokenized)\n",
    "\n",
    "#tfidfModel = IDF.fit(count_vectorized)\n",
    "#tfidf_df = tfidfModel.transform(count_vectorized)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create TF-IDF matrix\n",
    "idf_ngram = IDF().setInputCol('tf').setOutputCol('tfidf_ngram')\n",
    "tfidfModel_ngram = idf_ngram.fit(count_vectorized)\n",
    "tfidf_df_ngram = tfidfModel_ngram.transform(count_vectorized )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into training and testing set\n",
    "splits = tfidf_df_ngram .select(['tfidf_ngram', 'label']).randomSplit([0.8,0.2])\n",
    "train = splits[0].cache()\n",
    "test = splits[1].cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml import linalg as ml_linalg\n",
    "from pyspark.mllib.linalg import Vectors as MLLibVectors\n",
    "\n",
    "def as_mllib(v):\n",
    "    if isinstance(v, ml_linalg.SparseVector):\n",
    "        return MLLibVectors.sparse(v.size, v.indices, v.values)\n",
    "    elif isinstance(v, ml_linalg.DenseVector):\n",
    "        return MLLibVectors.dense(v.toArray())\n",
    "    else:\n",
    "        raise TypeError(\"Unsupported type: {0}\".format(type(v)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.mllib.util import MLUtils\n",
    "# Convert feature matrix to LabeledPoint vectors\n",
    "train_lb = train.rdd.map(lambda row: LabeledPoint(row[1], as_mllib(row[0])))\n",
    "test_lb = test.rdd.map(lambda row: LabeledPoint(row[1], as_mllib(row[0])))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# SVM model\n",
    "from pyspark.mllib.linalg import SparseVector\n",
    "from pyspark.mllib.regression import LabeledPoint\n",
    "from pyspark.mllib.classification import SVMWithSGD\n",
    "from numpy import array\n",
    "from pyspark.mllib.util import MLUtils\n",
    "numIterations = 100\n",
    "regParam = 0.3\n",
    "svm = SVMWithSGD.train(train_lb, numIterations, regParam=regParam)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model prediction\n",
    "scoreAndLabels_test = test_lb.map(lambda x: (float(svm.predict(x.features)), x.label))\n",
    "score_label_test = spark.createDataFrame(scoreAndLabels_test, [\"prediction\", \"label\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F1 score: 0.8848\n"
     ]
    }
   ],
   "source": [
    "# Model evaluation\n",
    "# F1 score\n",
    "from pyspark.ml.evaluation import MulticlassClassificationEvaluator\n",
    "f1_score = MulticlassClassificationEvaluator(labelCol=\"label\", predictionCol=\"prediction\", metricName=\"f1\")\n",
    "svm_f1 = f1_score.evaluate(score_label_test)\n",
    "print(\"F1 score: %.4f\" % svm_f1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
